# -*- coding: utf-8 -*-
"""Practical2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VDgShKVJgxpGxSQI4AZ2q9gmc79ko1dX
"""

!pip install gensim
import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec

documents = [
    "Machine learning is fun",
    "Deep learning is a part of machine learning",
    "Natural language processing uses machine learning"
]

count_vectorizer = CountVectorizer()
bow_count = count_vectorizer.fit_transform(documents)

bow_count_df = pd.DataFrame(
    bow_count.toarray(),
    columns=count_vectorizer.get_feature_names_out()
)

bow_count_df

normalized_bow = bow_count.toarray() / bow_count.toarray().sum(axis=1, keepdims=True)

normalized_bow_df = pd.DataFrame(
    normalized_bow,
    columns=count_vectorizer.get_feature_names_out()
)

normalized_bow_df

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=tfidf_vectorizer.get_feature_names_out()
)

tfidf_df

tokenized_docs = [doc.lower().split() for doc in documents]
tokenized_docs

word2vec_model = Word2Vec(
    sentences=tokenized_docs,
    vector_size=100,
    window=5,
    min_count=1,
    workers=4
)

def document_embedding(doc, model):
    vectors = [model.wv[word] for word in doc if word in model.wv]
    return np.mean(vectors, axis=0)

doc_embeddings = np.array([
    document_embedding(doc, word2vec_model) for doc in tokenized_docs
])

doc_embeddings